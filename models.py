""" Test different word embeddings on the malware data. """
import os
import csv
import time
from abc import ABC, abstractmethod
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import TextVectorization, GlobalAveragePooling1D, Dense, LSTM, GRU, Bidirectional, Embedding

def build_tf_params(threshold=0.5):
    tf_params = {}
    tf_params["optimizer"] = tf.keras.optimizers.Adam()
    tf_params["loss"] = tf.keras.losses.BinaryCrossentropy()
    tf_params["train_metrics"] = [tf.keras.metrics.BinaryAccuracy(threshold=threshold),
            tf.keras.metrics.Precision(thresholds=threshold, name='Precision'),
            tf.keras.metrics.Recall(thresholds=threshold, name='Recall')]
    tf_params["test_metrics"] = [tf.keras.metrics.BinaryAccuracy(threshold=threshold),
            tf.keras.metrics.Precision(thresholds=threshold, name='Precision'),
            tf.keras.metrics.Recall(thresholds=threshold, name='Recall')]
    return tf_params

class Model(ABC):

    line_flush = "\r\033[K"

    def __init__(self, tf_params, model_parameters, result_dir):
        self.optimizer = tf_params["optimizer"]
        self.loss = tf_params["loss"]
        self.train_metrics = tf_params["train_metrics"]
        self.test_metrics = tf_params["test_metrics"]
        self.tf_model = self.make_tf_model(model_parameters)
        self.result_dir = result_dir

    def _update_metrics(self, metrics, y, logits):
        metric_res = {}
        for metric in metrics:
            metric.update_state(y, logits)
            metric_res[metric.name] = metric.result().numpy()

        return metric_res

    def train_step(self, x, y):
        with tf.GradientTape() as tape:
            logits = self.tf_model(x, training=True)
            loss_value = self.loss(y, logits)

        grads = tape.gradient(loss_value, self.tf_model.trainable_weights)
        self.optimizer.apply_gradients(zip(grads, self.tf_model.trainable_weights))

        metric_res = self._update_metrics(self.train_metrics, y, logits)
        metric_res[self.loss.name] = loss_value.numpy()
        return metric_res

    def test_step(self, x, y):
        logits = self.tf_model(x, training=False)
        loss_value = self.loss(y, logits)

        metric_res = self._update_metrics(self.test_metrics, y, logits)
        metric_res[self.loss.name] = loss_value.numpy()
        return metric_res

    def online(self, batch, test_interval, max_api_seq):

        total = len(batch['apis'])
        print()

        for sample_idx, sample in enumerate(zip(batch['apis'], batch['labels']), start=1):
            api_seq = sample[0]
            label = [sample[1]]

            if sample_idx % test_interval == 0:
                start_time = time.time()
                metric_results = self.test_step(api_seq, label)
                end_time = time.time() - start_time 
                self.record_progress(batch['year'], "test", metric_results, end_time, batch['year'])
            else:
                start_time = time.time()
                metric_results = self.train_step(api_seq, label)
                end_time = time.time() - start_time 
                self.record_progress(batch['year'], "train", metric_results, end_time)

    def test(self, batch, test_interval, max_api_seq, train_year):

        total = len(batch['apis'])
        print()

        for sample_idx, sample in enumerate(zip(batch['apis'], batch['labels']), start=1):
            api_seq = sample[0]
            label = [sample[1]]

            if sample_idx % test_interval == 0:
                start_time = time.time()
                metric_results = self.test_step(api_seq, label)
                end_time = time.time() - start_time 
                self.record_progress(train_year, "test", metric_results, end_time, batch['year'])

    def batch_train(self, train_x, train_y, year, batch_size=32):

        def chunk_data(data, chunk_size):
            for i in range(0, len(data), chunk_size):
                yield data[i:i+chunk_size]

        chunk_train_x = list(chunk_data(train_x, batch_size))
        chunk_train_y = list(chunk_data(train_y, batch_size))

        for chunk_x, chunk_y in zip(chunk_train_x, chunk_train_y):
            chunk_x = tf.convert_to_tensor(chunk_x)
            start_time = time.time()
            metric_results = self.train_step(chunk_x, chunk_y)
            end_time = time.time() - start_time 
            self.record_progress(year, "train", metric_results, end_time)

    def record_progress(self, train_year, model_status, metric_results, processing_time, test_year=""):

        save_file = f"{self.tf_model.name}-{train_year}-{model_status}{test_year}.csv"

        if not os.path.isdir(self.result_dir):
            os.mkdir(self.result_dir)

        header = list(metric_results.keys())
        header.append("Time")

        content = list(metric_results.values())
        content.append(processing_time)

        save_file_path = os.path.join(self.result_dir, save_file)
        is_res_file = os.path.isfile(save_file_path)

        with open(save_file_path, 'a', newline='') as csvfile:
            resultwriter = csv.writer(csvfile)

            if not is_res_file:  # Only add header if new file.
                resultwriter.writerow(header)

            resultwriter.writerow(content)

    @abstractmethod
    def make_tf_model(self, model_parameters: dict):
        raise NotImplementedError("You should implement this!")

def make_text_layer(model_parameters):

    # The plus 2 on max tokens compensates for the empty and unknown tokens.
    vectorizer = TextVectorization(max_tokens=len(model_parameters['vocabulary'])+2,
                                    standardize=None, split="whitespace", output_mode="int",
                                    output_sequence_length=model_parameters['max_api_seq'],
                                    vocabulary=model_parameters['vocabulary'])

    emb_layer = Embedding(vectorizer.vocabulary_size(),
            model_parameters['embedding_dim'], name="embedding")

    return vectorizer, emb_layer

class SEM(Model):
    def make_tf_model(self, model_parameters):

        vectorizer, emb_layer = make_text_layer(model_parameters)

        model = Sequential(name="SEM")
        model.add(vectorizer)
        model.add(emb_layer)
        model.add(GlobalAveragePooling1D())
        model.add(Dense(model_parameters['number_of_outputs'], activation="sigmoid"))
        return model

class TextMLP(Model):
    def make_tf_model(self, model_parameters):

        vectorizer, emb_layer = make_text_layer(model_parameters)
        regularizer = tf.keras.regularizers.L1L2(l1=model_parameters['lasso'], l2=model_parameters['ridge'])

        model = Sequential(name=f"MLP-L1:{model_parameters['lasso']}-L2:{model_parameters['ridge']}")
        model.add(vectorizer)
        model.add(emb_layer)
        model.add(GlobalAveragePooling1D())
        model.add(Dense(64, activation="relu", kernel_regularizer=regularizer))
        model.add(Dense(model_parameters['number_of_outputs'], activation="sigmoid"))
        return model

class TextLSTM(Model):
    def make_tf_model(self, model_parameters):

        vectorizer, emb_layer = make_text_layer(model_parameters)
        regularizer = tf.keras.regularizers.L1L2(l1=model_parameters['lasso'], l2=model_parameters['ridge'])

        model = Sequential(name=f"LSTM-L1:{model_parameters['lasso']}-L2:{model_parameters['ridge']}")
        model.add(vectorizer)
        model.add(emb_layer)
        model.add(LSTM(64, batch_input_shape=(model_parameters['batch_size'],
            model_parameters['max_api_seq'],
            model_parameters['embedding_dim']), kernel_regularizer=regularizer))
        model.add(Dense(64, activation="relu"))
        model.add(Dense(model_parameters['number_of_outputs'], activation="sigmoid"))
        return model

class TextGRU(Model):
    def make_tf_model(self, model_parameters):

        vectorizer, emb_layer = make_text_layer(model_parameters)
        regularizer = tf.keras.regularizers.L1L2(l1=model_parameters['lasso'], l2=model_parameters['ridge'])

        model = Sequential(name=f"GRU-L1:{model_parameters['lasso']}-L2:{model_parameters['ridge']}")
        model.add(vectorizer)
        model.add(emb_layer)
        model.add(GRU(64, batch_input_shape=(model_parameters['batch_size'],
            model_parameters['max_api_seq'],
            model_parameters['embedding_dim']), kernel_regularizer=regularizer))
        model.add(Dense(64, activation="relu"))
        model.add(Dense(model_parameters['number_of_outputs'], activation="sigmoid"))
        return model

class TextBiLSTM(Model):
    def make_tf_model(self, model_parameters):

        vectorizer, emb_layer = make_text_layer(model_parameters)
        regularizer = tf.keras.regularizers.L1L2(l1=model_parameters['lasso'], l2=model_parameters['ridge'])

        model = Sequential(name=f"BiLSTM-L1:{model_parameters['lasso']}-L2:{model_parameters['ridge']}")
        model.add(vectorizer)
        model.add(emb_layer)
        model.add(Bidirectional(LSTM(64, batch_input_shape=(model_parameters['batch_size'],
            model_parameters['max_api_seq'],
            model_parameters['embedding_dim']), kernel_regularizer=regularizer)))
        model.add(Dense(64, activation="relu"))
        model.add(Dense(model_parameters['number_of_outputs'], activation="sigmoid"))
        return model

class TextBiGRU(Model):
    def make_tf_model(self, model_parameters):

        vectorizer, emb_layer = make_text_layer(model_parameters)
        regularizer = tf.keras.regularizers.L1L2(l1=model_parameters['lasso'], l2=model_parameters['ridge'])

        model = Sequential(name=f"BiGRU-L1:{model_parameters['lasso']}-L2:{model_parameters['ridge']}")
        model.add(vectorizer)
        model.add(emb_layer)
        model.add(Bidirectional(GRU(64, batch_input_shape=(model_parameters['batch_size'],
            model_parameters['max_api_seq'],
            model_parameters['embedding_dim']), kernel_regularizer=regularizer)))
        model.add(Dense(64, activation="relu"))
        model.add(Dense(model_parameters['number_of_outputs'], activation="sigmoid"))
        return model
