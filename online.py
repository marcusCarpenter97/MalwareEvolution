import os
import csv
import time
import tensorflow as tf

def tensor_to_npfloat(tensor):
    return tensor.numpy()

def progress_logger(sample_idx, loss, acc, training, model_name):
    """
    sample_idx : index for current sample.
    loss : loss value of current sample.
    acc : accuracy score for current sample.
    training : 'True' or 'False' (not boolean, string). Was the model in training mode?
    """
    res_file = f'results-{model_name}.csv'
    header = ['Sample', 'Loss', 'Accuracy', 'Training']
    is_res_file = os.path.isfile(res_file)

    with open(res_file, 'a', newline='') as csvfile:
        resultwriter = csv.writer(csvfile)

        if not is_res_file:  # Only add header if new file.
            resultwriter.writerow(header)

        resultwriter.writerow([sample_idx, loss, acc, training])

#@tf.function
def train_step(x, y, model, loss_fn, optimizer, train_metric):
    with tf.GradientTape() as tape:
        logits = model(x, training=True)
        loss_value = loss_fn(y, logits)

    grads = tape.gradient(loss_value, model.trainable_weights)
    optimizer.apply_gradients(zip(grads, model.trainable_weights))
    train_metric.update_state(y, logits)
    return loss_value

#@tf.function
def test_step(x, y, model, loss_fn, test_metric):
    logits = model(x, training=False)
    loss_value = loss_fn(y, logits)
    test_metric.update_state(y, logits)
    return loss_value

def train_model(data, model, loss_fn, optimizer, train_metric, test_metric, test_interval):
    """
    Custom training loop for Tensorflow models.

    data : Tensorflow dataset.
    model : Tensorflow model.
    loss_fn : Tensorflow loss function.
    optimizer : Tensorflow optimizer.
    train_metric : Tensorflow metric (or list of metrics).
    val_metric : Tensorflow metric (or list of metrics).
    test_interval : Integer. Test model every nth sample.
    """
    print(f"\nModel being trained: {model.name}")
    start_time = time.time()

    for step, (x, y) in enumerate(data):
        if step > 0 and step % test_interval == 0:  # Interlace test with training for online learning.
            val_loss = test_step(x, y, model, loss_fn, test_metric)
            val_acc = test_metric.result()
            test_metric.reset_states()  # TODO optional
            val_loss = tensor_to_npfloat(val_loss)
            val_acc = tensor_to_npfloat(val_acc)
            progress_logger(step, val_loss, val_acc, 'False', model.name)
        else:
            # This concrete function is required to change optimizers.
            #train_func = tf.function(train_step).get_concrete_function(x, y, model, loss_fn, optimizer, train_metric)
            #train_loss = train_func(x, y, model, loss_fn, train_metric)
            train_loss = train_step(x, y, model, loss_fn, optimizer, train_metric)
            train_acc = train_metric.result()
            train_metric.reset_states()  # TODO optional
            train_loss = tensor_to_npfloat(train_loss)
            train_acc = tensor_to_npfloat(train_acc)
            progress_logger(step, train_loss, train_acc, 'True', model.name)

    print("Time taken: %.2fs" % (time.time() - start_time))
