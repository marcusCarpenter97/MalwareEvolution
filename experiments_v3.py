"""Compare online learning methods for training models."""
import os
import models
import backend

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import tensorflow as tf

def create_model_pipeline(exp_name, model_parameters):

    sem = models.SEM(models.build_tf_params(), model_parameters, exp_name)

    model_parameters['lasso'] = 0.81
    model_parameters['ridge'] = 0.81
    mlp = models.TextMLP(models.build_tf_params(), model_parameters, exp_name)

    model_parameters['lasso'] = 0.0
    model_parameters['ridge'] = 0.21
    bilstm = models.TextBiLSTM(models.build_tf_params(), model_parameters, exp_name)

    model_pipeline = [sem, mlp, bilstm]

    return model_pipeline

def calculate_test_interval(apis, test_ratio):
    test_size = int(len(apis) * test_ratio)
    return int(len(apis) / test_size) + 1

def one_task_one_sample(model, data, model_parameters):
    print(f"Training model: {model.tf_model.name} as {model.result_dir}")

    for batch_idx, batch in enumerate(data):
        print(f"On batch {batch['year']}")

        test_interval = calculate_test_interval(batch['apis'], model_parameters['test_ratio'])

        model.online(batch, test_interval, model_parameters['max_api_seq'])

        for test_batch_idx, test_batch in enumerate(data):
            if test_batch_idx == batch_idx:
                continue
            sub_test_interval = calculate_test_interval(test_batch['apis'],
                                                        model_parameters['test_ratio'])

            model.test(test_batch, sub_test_interval,
                       model_parameters['max_api_seq'], batch['year'])

def one_task_one_batch(model, data, model_parameters):
    print(f"Training model: {model.tf_model.name} as {model.result_dir}")

    for batch_idx, batch in enumerate(data):
        print(f"On batch {batch['year']} with {len(batch['apis'])} samples")
        train_x, test_x, train_y, test_y = backend.split_train_test(batch['apis'], batch['labels'],
                                                                    test_size=model_parameters['test_ratio'])
        model.batch_train(train_x, train_y, batch['year'])

        test_batch = {}
        test_batch['year'] = batch['year']
        test_batch['apis'] = test_x
        test_batch['labels'] = test_y
        model.test(test_batch, 1, 0, batch['year'])

        for test_batch_idx, test_batch in enumerate(data):
            if test_batch_idx == batch_idx:
                continue
            # 1 = test on all samples, 0 = max_api_seq is deprecated and irrelevant.
            model.test(test_batch, 1, 0, batch['year'])

def one_task_one_data(data, model_parameters, save_dir):

    print(f"\nSAVE DIR: {save_dir}\n")

    train_x_sets = {}
    train_y_sets = {}

    test_x_sets = {}
    test_y_sets = {}

    for batch in data:
        train_x, test_x, train_y, test_y = backend.split_train_test(batch['apis'], batch['labels'],
                                                                    test_size=model_parameters['test_ratio'])
        if not train_x_sets.keys():
            prev_year = batch['year']
            train_x_sets[batch['year']] = train_x
            train_y_sets[batch['year']] = train_y
        else:
            train_x_sets[batch['year']] = train_x_sets[prev_year] + train_x
            train_y_sets[batch['year']] = train_y_sets[prev_year] + train_y
            prev_year = batch['year']

        test_x_sets[batch['year']] = test_x
        test_y_sets[batch['year']] = test_y

    for train_year in train_x_sets:

        model_pipeline = create_model_pipeline(save_dir, model_parameters)

        for model in model_pipeline:
            print(f"TRAINING year: {train_year} - model: {model.tf_model.name}")
            model.batch_train(train_x_sets[train_year], train_y_sets[train_year], train_year)
            #print("train_x", len(train_x_sets[year]))
            #print("train_y", len(train_y_sets[year]))

            for test_year in test_x_sets:
                print(f"TESTING year: {test_year} - model: {model.tf_model.name}")
                #print("test_x", len(test_x_sets[year]))
                #print("test_y", len(test_y_sets[year]))
                test_set = {}
                test_set['apis'] = test_x_sets[test_year]
                test_set['labels'] = test_y_sets[test_year]
                test_set['year'] = test_year
                model.test(test_set, 1, 0, train_year)

if __name__ == "__main__":

    result_dir = "online_comparison"
    sub_dirs = ["one-task-one-sample", "one-task-one-batch", "one-task-one-data"]

    # Load data sequentialy
    raw_data = backend.load_data(backend.DATA_DIR)
    raw_data = sorted(raw_data, key=lambda b: b['year'])

    for batch in raw_data:
        batch['apis'] = backend.trim_data([batch], mode='first')

    model_parameters = {}
    model_parameters['max_api_seq'] = 512   # Also number of timesteps
    model_parameters['test_ratio'] = 0.15   # Size of test data based on whole dataset
    model_parameters['embedding_dim'] = 16  # Also number of feature in the model
    model_parameters['batch_size'] = 1      # Only one sample when online
    model_parameters['number_of_outputs'] = len(raw_data[0]['labels'][0])
    model_parameters['vocabulary'] = list({api for batch in raw_data for sample in batch['apis'] for api in sample})

    for batch in raw_data:
        for idx, sample in enumerate(batch['apis']):
            batch['apis'][idx] = tf.convert_to_tensor([" ".join(sample)])

    #one_sample_pipe = create_model_pipeline(os.path.join(result_dir, sub_dirs[0]), model_parameters)
    #one_batch_pipe = create_model_pipeline(os.path.join(result_dir, sub_dirs[1]), model_parameters)

    #for model in one_sample_pipe:
    #    one_task_one_sample(model, raw_data, model_parameters)

    #for model in one_batch_pipe:
    #    one_task_one_batch(model, raw_data, model_parameters)

    one_task_one_data(raw_data, model_parameters, os.path.join(result_dir, sub_dirs[2]))
