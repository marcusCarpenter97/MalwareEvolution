import re
import os
from tabulate import tabulate
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

def calculate_f1(precision, recall):
    return 2 * (precision * recall) / (precision + recall)

def make_columns(column_files):

    f1_mean_column = []
    f1_std_column = []

    for column_file in column_files:
        result_data = pd.read_csv(column_file)
        f1_column = calculate_f1(result_data['Precision'], result_data['Recall'])

        f1_mean = f1_column.mean()
        f1_std = f1_column.std()

        f1_mean_column.append(f1_mean)
        f1_std_column.append(f1_std)
    return f1_mean_column, f1_std_column

def make_tables(table_files):

    col_size = 3
    table_size = len(table_files)
    table_files = [table_files[idx:idx+col_size] for idx in range(0, table_size, col_size)]

    f1_mean_table = {}
    f1_std_table = {}

    years = []

    for column_files in table_files:
        f1_mean_column, f1_std_column = make_columns(column_files)
        year = re.search('-\d{4}-', column_files[0]).group(0).strip('-')  # This assumes a specific naming convention.

        f1_mean_table[year] = f1_mean_column
        f1_std_table[year] = f1_std_column

        years.append(year)

    model_name = table_files[0][0][11:-18]  # This makes assumptions on the file name.

    f1_mean_table = pd.DataFrame(data=f1_mean_table, index=years).rename_axis(model_name, axis=1)
    f1_std_table = pd.DataFrame(data=f1_std_table, index=years).rename_axis(model_name, axis=1)
    return f1_mean_table, f1_std_table

def compile_results(result_dir="result_dir"):

    table_size = 9

    result_files = os.listdir(result_dir)
    result_files = sorted([i for i in result_files if "train" not in i])  # Remove training results
    result_files = [os.path.join(result_dir, res_file) for res_file in result_files]

    num_files = len(result_files)
    result_files = [result_files[idx:idx+table_size] for idx in range(0, num_files, table_size)]

    f1_mean_tables = []
    f1_std_tables = []

    for table_files in result_files:
        f1_mean_table, f1_std_table = make_tables(table_files)

        f1_mean_tables.append(f1_mean_table)
        f1_std_tables.append(f1_std_table)

    return f1_mean_tables, f1_std_tables

def find_best_models_overall(f1_mean_tables, f1_std_tables, n_ranks):

    rank = list(zip(f1_mean_tables, f1_std_tables))
    # Sort by f1 descending with std for tie break, use mean of table to sort
    rank.sort(key=lambda x: (-x[0].mean(axis=None), x[1].mean(axis=None)))
    return rank[:n_ranks]

def find_best_models_memory(f1_mean_tables, f1_std_tables, n_ranks):

    rank = list(zip(f1_mean_tables, f1_std_tables))
    # Sort by f1 descending with std for tie break, use mean of table to sort
    rank.sort(key=lambda x: (-x[0]['2022'].mean(axis=None), x[1]['2022'].mean(axis=None)))
    return rank[:n_ranks]

def make_model_train_test_table(mean_table, std_table, model_name):

    headers = mean_table.columns
    table = []
    print(model_name)
    for row_idx in range(len(mean_table.index)):
        table_row = [mean_table.index[row_idx]]
        for col_idx in range(len(mean_table.columns)):
            fone = mean_table.iat[row_idx, col_idx]
            std = std_table.iat[row_idx, col_idx]
            table_row.append(f"{fone} \pm {std}")
        table.append(table_row)
    print(tabulate(table, headers, tablefmt="latex_raw"))

def plot_rank_with_std(model_rank, plot_name):
    plot_path = os.path.join("rank-plots", f"{plot_name}.png")

    f1_rank = []
    std_rank = []
    for model in model_rank:
        f1_rank.append(model[0].mean(axis=None))
        std_rank.append(model[1].mean(axis=None))

    bar_coords = np.arange(len(f1_rank))

    fig, axis = plt.subplots()
    axis.errorbar(x=bar_coords, y=f1_rank, yerr=std_rank)

    axis.set_title(plot_name)
    axis.set_ylabel("F1")
    axis.set_xlabel("Models")
    fig.savefig(plot_path)

def make_model_rank_table(model_rank):

    header = ["Model", "F1-score overall", "F1-score 2022"]

    table = []
    for model in model_rank:
        fone_mean = model[0].mean(axis=None)
        std_mean = model[1].mean(axis=None)
        overall_col = f"${fone_mean} \pm {std_mean}$"

        fone_twotwo = model[0]['2022'].mean(axis=None)
        std_twotwo = model[1]['2022'].mean(axis=None)
        twotwo_col = f"${fone_twotwo} \pm {std_twotwo}$"

        table.append([model[0].columns.name, overall_col, twotwo_col])

    print(tabulate(table, header, tablefmt="latex_raw"))

def plot_model_loss(result_files, result_dir):

    loss = {}
    x_ticks = []
    plot_title = result_files[0][:-13]
    prev_len = 0
    for result_name in result_files:
        file_path = os.path.join(result_dir, result_name)
        df = pd.read_csv(file_path)
        test_year = result_name[-8:-4]
        data = df['binary_crossentropy']

        nans = pd.Series([np.nan] * prev_len)
        final = pd.concat([nans, data], ignore_index=True)

        x_ticks.append(prev_len)
        prev_len += data.size
        loss[test_year] = final

    fig, axis = plt.subplots()
    for year in loss:
        axis.plot(loss[year])
    axis.set_xticks(x_ticks)
    axis.set_xlabel("Samples")
    axis.set_ylabel("Binary Crossentropy")
    axis.legend(loss.keys())
    axis.set_title(plot_title)

    fig.savefig(f"plots/{plot_title}.png")

def find_model(model_name, f1_mean_tables):
    return next(filter(lambda table: table.columns.name == model_name, f1_mean_tables))

def make_tables_for_online_comparison(result_dir="online_comparison"):
    model_names = ["BiLSTM-L1:0.0-L2:0.21", "SEM", "MLP-L1:0.81-L2:0.81"]
    name_reg = re.compile("|".join(model_names))
    experiment_dirs = [os.path.join(result_dir, i) for i in os.listdir(result_dir)]

    for experiment_dir in experiment_dirs:
        print(experiment_dir)
        f1_mean_tables, f1_std_tables = compile_results(experiment_dir)

        for mean_table, std_table in zip(f1_mean_tables, f1_std_tables):
            model_name = name_reg.search(mean_table.columns.name)
            if model_name is None:
                continue
            model_name = model_name.group()
            make_model_train_test_table(mean_table, std_table, model_name)

if __name__ == "__main__":

    make_tables_for_online_comparison()

    #for model_name in model_names:
    #    f1_results = find_model(model_name, f1_mean_tables)
    #    std_results = find_model(model_name, f1_std_tables)

    #    make_model_train_test_table(f1_results, std_results, model_name)

    #top_n_models = 10
    #best_overall = find_best_models_overall(f1_mean_tables, f1_std_tables, top_n_models)
    #best_memory = find_best_models_memory(f1_mean_tables, f1_std_tables, top_n_models)

    #make_model_rank_table(best_overall)
    #make_model_rank_table(best_memory)

    # File picker
    #target_names = ["BiLSTM-L1:0.0-L2:0.21-2022-test", "SEM-2022-test", "MLP-L1:0.81-L2:0.81-2022-test", "BiLSTM-L1:0.0-L2:0.0-2022-test", "MLP-L1:0.0-L2:0.0-2022-test"]
    #for target_name in target_names:
    #    target_files = sorted([f for f in os.listdir(result_dir) if target_name in f])
    #    plot_model_loss(target_files, result_dir)

