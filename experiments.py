import tensorflow as tf
#from tensorflow.keras.layers import TextVectorization
from matplotlib import pyplot as plt
import models
import backend
import online
#import pipeline

def load_data():
    """ Wraper for backend module. """
    raw_data = backend.load_data(backend.DATA_DIR)
    trimmed_apis = backend.trim_data(raw_data, mode='first')
    trimmed_strs = [" ".join(api_seq) for api_seq in trimmed_apis]
    labels = backend.get_labels(raw_data)
    return trimmed_strs, labels

def make_plot(data, xlabel, ylabel, title, figname):
    fig, ax = plt.subplots()
    ax.plot(data)
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    ax.set_title(title)
    fig.savefig(figname)

if __name__ == "__main__":
    batch_size = 1

    x_data, y_data = load_data()
    tf_data = tf.data.Dataset.from_tensor_slices((x_data, y_data)).batch(batch_size)

    # The full vocabulary is know to the model.
    # This is the equivalent as a new model  knowing all possible API calls.
    # This makes sense becuase there is a finite number of APIs available.
    # All od which can be viewd in the documentation of the targated system.
    full_corpus = tf.data.Dataset.from_tensor_slices(x_data)
    vectorizer = models.train_vectorization_layer(full_corpus)
    vocab_size = vectorizer.vocabulary_size()

    embedding_dim = 16
    label_size = 15

    test_interval = 5

    n_timesteps = 1
    n_features = 16

    model_pipeline = [models.simple_embedding_model(vectorizer, vocab_size, embedding_dim, label_size),
                      models.multilayer_perceptron_model(vectorizer, vocab_size, embedding_dim, label_size),
                      models.lstm_model(vectorizer, vocab_size, embedding_dim, batch_size, n_timesteps, n_features, label_size),
                      models.gru_model(vectorizer, vocab_size, embedding_dim, batch_size, n_timesteps, n_features, label_size),
                      models.bi_lstm_model(vectorizer, vocab_size, embedding_dim, batch_size, n_timesteps, n_features, label_size),
                      models.bi_gru_model(vectorizer, vocab_size, embedding_dim, batch_size, n_timesteps, n_features, label_size)]

    for model in model_pipeline:

        optimizer = tf.keras.optimizers.Adam(global_clipnorm=1.0)
        loss = tf.keras.losses.BinaryCrossentropy()
        train_metric = tf.keras.metrics.BinaryAccuracy(threshold=0.5)
        test_metric = tf.keras.metrics.BinaryAccuracy(threshold=0.5)

        online.train_model(tf_data, model, loss, optimizer, train_metric, test_metric, test_interval)
#        train_loss = []
#        train_accuracy = []
#
#        eval_loss = []
#        eval_accuracy = []

        #for iteration in range(500): # TODO for testing, use all samples eventually
            #sample = pipe.get_next_data_item()  # TODO Iterator does not work becuase it will be used up.

            #if not sample:
            #    break
            #raw_data = pipe.trimmed_APIs[iteration]
            #expected_output = pipe.labels[iteration]

            # Unpack data from pipe.
            #input_api_seq = [" ".join(raw_data)]
            #expected_output = sample[1]

#            vocabulary.update(input_api_seq)
#
#            vectorizer.set_vocabulary(list(vocabulary))
#
#            if iteration == 0:
#                model.compile(optimizer=optimizer, loss=tf.keras.losses.BinaryCrossentropy(),
#                        metrics=[tf.keras.metrics.BinaryAccuracy(threshold=0.5)])
#
#            # Train model on new sample. Evaluate every fith sample.
#            if iteration % 5 == 0:
#                eval_results = model.evaluate(x=input_api_seq, y=[expected_output], verbose=0)
#                eval_loss.append(eval_results[0])
#                eval_accuracy.append(eval_results[1])
#            else:
#                history = model.fit(x=input_api_seq, y=[expected_output], epochs=1, verbose=0)
#                train_loss.extend(history.history['loss'])
#                train_accuracy.extend(history.history['binary_accuracy'])
#
#            print(iteration, end="\r")

        #make_plot(train_loss, "epoch", "loss", f"Loss of {model.name}", f"trainLoss{model.name}.png")
        #make_plot(train_accuracy, "epoch", "binary accuracy", f"Binary accuracy of {model.name}", f"trainAccuracy{model.name}.png")
        #make_plot(eval_loss, "epoch", "loss", f"Loss of {model.name}", f"evalLoss{model.name}.png")
        #make_plot(eval_accuracy, "epoch", "binary accuracy", f"Binary accuracy of {model.name}", f"evalAccuracy{model.name}.png")

        # TODO list
        # Call model.fit() on one sample. Any adjustments to the models?
        # How to test performance? Compile all metrics and make tables and plots when pipe is empty.

        # What will the experiments be?
        # Pretrained vs online only?
        # Pretrain vectorisation?
        # Test every five samples. So train on four and predicts on fith.
        # How to compute and log regret?

        # Training history sounds important.
        # Choose metrics to evaluate models on. Choose loss function.

        # Final results must show how the models performs over time, the objective is to minimize regret.
