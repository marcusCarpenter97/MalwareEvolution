import os
import csv
import time
import tensorflow as tf
from tensorflow.keras.layers import TextVectorization, Embedding
import models
import backend
#import online

def progress_logger(sample_idx, loss, acc, training, model_name):
    """
    sample_idx : index for current sample.
    loss : loss value of current sample.
    acc : accuracy score for current sample.
    training : 'True' or 'False' (not boolean, string). Was the model in training mode?
    """
    res_file = f'results-{model_name}.csv'
    header = ['Sample', 'Loss', 'Accuracy', 'Training']
    is_res_file = os.path.isfile(res_file)

    with open(res_file, 'a', newline='') as csvfile:
        resultwriter = csv.writer(csvfile)

        if not is_res_file:  # Only add header if new file.
            resultwriter.writerow(header)

        resultwriter.writerow([sample_idx, loss, acc, training])

#@tf.function
def train_step(x, y, model, loss_fn, optimizer, train_metric):
    with tf.GradientTape() as tape:
        logits = model(x, training=True)
        loss_value = loss_fn(y, logits)

    grads = tape.gradient(loss_value, model.trainable_weights)
    optimizer.apply_gradients(zip(grads, model.trainable_weights))
    train_metric.update_state(y, logits)
    return loss_value

#@tf.function
def test_step(x, y, model, loss_fn, test_metric):
    logits = model(x, training=False)
    loss_value = loss_fn(y, logits)
    test_metric.update_state(y, logits)
    return loss_value

def add_vocab_tokens(vocab):
    temp_vocab = vocab.copy()
    temp_vocab.insert(0, "")
    temp_vocab.insert(0, "[UNK]")
    return temp_vocab

if __name__ == "__main__":

    batch_size = 1     # Only one sample when online
    n_timesteps = 512  # Number of APIs in sequence
    output_dim = 15    # number of bits in label
    n_features = 16    # Arbitary size of embeddings
    embedding_dim = 16

    classifier_heads = [models.simple_embedding_model(output_dim),
                        models.multilayer_perceptron_model(output_dim),
                        models.lstm_model(output_dim, batch_size, n_timesteps, n_features),
                        models.gru_model(output_dim, batch_size, n_timesteps, n_features),
                        models.bi_lstm_model(output_dim, batch_size, n_timesteps, n_features),
                        models.bi_gru_model(output_dim, batch_size, n_timesteps, n_features)]

    # Load data sequentialy
    raw_data = backend.load_data(backend.DATA_DIR)
    raw_data = sorted(raw_data, key=lambda d: d['year'])

    for batch in raw_data:
        trimmed_apis = backend.trim_data([batch], mode='first')

        train_x, test_x, train_y, test_y = backend.split_train_test(trimmed_apis, batch['labels'], test_size=0.15)

        for classifier in classifier_heads:
            base_vocabulary = []

            train_counter = 1
            test_counter = 1
            start_time = time.time()

            optimizer = tf.keras.optimizers.legacy.Adam()  # TODO optimizers to test SGD, OGD, Adam.
            loss = tf.keras.losses.BinaryCrossentropy()
            train_metric = tf.keras.metrics.BinaryAccuracy(threshold=0.5)
            test_metric = tf.keras.metrics.BinaryAccuracy(threshold=0.5)

            for sample, label in zip(train_x, train_y):

                input_api_seq = [" ".join(sample)]  # List of one String

                # Update vocabulary
                new_vocabulary = list(set(base_vocabulary).union(sample))

                vectorizer = TextVectorization(standardize="lower", output_mode="int", vocabulary=new_vocabulary, name="text_vectorization")

                # New Embeddings
                emb_layer = Embedding(vectorizer.vocabulary_size(), embedding_dim, name="embedding")

                if train_counter > 1:
                    updated_embeddings = tf.keras.utils.warmstart_embedding_matrix(
                            base_vocabulary = add_vocab_tokens(base_vocabulary),  # from old model
                            new_vocabulary = add_vocab_tokens(new_vocabulary),    # from new data to build new model
                            base_embeddings = base_embedding_weights,
                            new_embeddings_initializer="uniform")

                    emb_layer.build(input_shape=[None])
                    emb_layer.embeddings.assign(updated_embeddings)

                # Text processing pipeline
                text_input_layer = tf.keras.Sequential([vectorizer, emb_layer], name="text_input_layer")

                # Add text processing to classifier model
                model = tf.keras.Sequential([text_input_layer, classifier], name=f"full_{classifier.name}")

                # Train model
                x = tf.constant(input_api_seq, dtype=tf.string, shape=(1,))
                y = tf.constant(label, shape=(1,15))

                train_loss = train_step(x, y, model, loss, optimizer, train_metric)
                progress_logger(train_counter, train_loss, train_metric.result(), 'True',  classifier.name)
                print(f"Trained {classifier.name} on sample number: {train_counter} with Binary Accuracy: {train_metric.result().numpy()}", end="\r")

                # Save embeddings from current iteration
                base_embedding_weights = model.get_layer("text_input_layer").get_layer("embedding").get_weights()[0]
                base_vocabulary = new_vocabulary.copy()
                train_counter +=1
            print(f"Time taken to train {classifier.name}: %.2fs" % (time.time() - start_time))  

            start_time = time.time()
            print(f"Begin testing phase on {len(test_x)} samples.")
            for sample, label in zip(test_x, test_y):
                input_api_seq = [" ".join(sample)]  # List of one String
                x = tf.constant(input_api_seq, dtype=tf.string, shape=(1,))
                y = tf.constant(label, shape=(1,15))

                # Uses the last model generated by the training phase.
                test_loss = test_step(x, y, model, loss, test_metric)
                progress_logger(test_counter, train_loss, train_metric.result(), 'False', classifier.name)
                test_counter +=1
            print(f"Time taken to test {classifier.name}: %.2fs" % (time.time() - start_time))  
