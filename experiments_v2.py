"""Experiments to compare the performance of models and their regularisers."""
import numpy as np
import tensorflow as tf
import models
import backend

def build_tf_params(threshold=0.5):
    tf_params = {}
    tf_params["optimizer"] = tf.keras.optimizers.Adam()
    tf_params["loss"] = tf.keras.losses.BinaryCrossentropy()
    tf_params["train_metrics"] = [tf.keras.metrics.BinaryAccuracy(threshold=threshold),
            tf.keras.metrics.Precision(thresholds=threshold, name='Precision'),
            tf.keras.metrics.Recall(thresholds=threshold, name='Recall')]
    tf_params["test_metrics"] = [tf.keras.metrics.BinaryAccuracy(threshold=threshold),
            tf.keras.metrics.Precision(thresholds=threshold, name='Precision'),
            tf.keras.metrics.Recall(thresholds=threshold, name='Recall')]
    return tf_params

def split_train_test(data, interval):
    test = data[::interval]
    train = data.copy()
    del train[::interval]
    return train, test

def calculate_test_interval(apis, test_ratio):
    test_size = int(len(apis) * test_ratio)
    return int(len(apis) / test_size) + 1

def generate_regularizer_params(num_trials, max_reg=1, interval=0.1):
    distribution = np.around(np.arange(0.01, max_reg, interval), decimals=2)
    zeros = np.zeros(num_trials)
    l = np.concatenate([distribution, zeros, distribution])  # L1 (lasso)
    r = np.concatenate([zeros, distribution, distribution])  # L2 (ridge)
    regs = np.array([l, r]).T
    return np.insert(regs, 0, [0,0], axis=0)

def generate_regularizer_models(model_params, reg_params):
    model_pipe = []
    for reg in reg_params:
        model_params['lasso'] = reg[0]
        model_params['ridge'] = reg[1]
        model_pipe.append(models.TextMLP(build_tf_params(), model_params))
        model_pipe.append(models.TextLSTM(build_tf_params(), model_params))
        model_pipe.append(models.TextGRU(build_tf_params(), model_params))
        model_pipe.append(models.TextBiLSTM(build_tf_params(), model_params))
        model_pipe.append(models.TextBiGRU(build_tf_params(), model_params))
    return model_pipe

def run_experiments(raw_data, model_pipeline, model_parameters):

    for model in model_pipeline:
        for batch_idx, batch in enumerate(raw_data):

            test_interval = calculate_test_interval(batch['apis'], model_parameters['test_ratio'])

            model.online(batch, test_interval, model_parameters['max_api_seq'])

            for test_batch_idx, test_batch in enumerate(raw_data):
                if test_batch_idx == batch_idx:
                    continue

                sub_test_interval = calculate_test_interval(test_batch['apis'], model_parameters['test_ratio'])

                model.test(test_batch, sub_test_interval, model_parameters['max_api_seq'], batch['year'])

if __name__ == "__main__":

    model_parameters = {}
    model_parameters['max_api_seq']   = 512   # Also number of timesteps
    model_parameters['test_ratio']    = 0.15  # Size of test data based on whole dataset
    model_parameters['embedding_dim'] = 16    # Also number of feature in the model
    model_parameters['batch_size']    = 1     # Only one sample when online

    # Load data sequentialy
    raw_data = backend.load_data(backend.DATA_DIR)
    raw_data = sorted(raw_data, key=lambda b: b['year'])

    model_parameters['number_of_outputs'] = len(raw_data[0]['labels'][0])
    model_parameters['vocabulary'] = list({api for batch in raw_data for sample in batch['apis'] for api in sample})

    regularizer_params = generate_regularizer_params(10)
    model_pipeline = generate_regularizer_models(model_parameters, regularizer_params)

    model_pipeline.insert(0, models.SEM(build_tf_params(), model_parameters))

    print("---------------")
    for reg in regularizer_params:
        print(f"Lasso: {reg[0]} - Ridge: {reg[1]}")
    print("---------------")
    print("Model pipeline size:", len(model_pipeline))

    run_experiments(raw_data, model_pipeline, model_parameters)

